{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim : To classify sentiment of movie reviews in imdb dataset as positive or negative. \n",
    "\n",
    "### Results : Please check at the end of notebook\n",
    "\n",
    "#### Tools used: Keras, imdb dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1:** import required python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Use Tensorflow backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "if os.environ[\"KERAS_BACKEND\"] != \"tensorflow\" :\n",
    "    #To use MKL 2018 with Theano you MUST set \"MKL_THREADING_LAYER=GNU\" in your environement.\n",
    "    os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "\n",
    "from keras.datasets      import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras               import callbacks\n",
    "from keras.layers        import Embedding,LSTM,Dense\n",
    "from keras.models        import Sequential,load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2:** Load the IMDB dataset\n",
    "\n",
    "IMDB reviews are already pre-processed in keras. Each review is stored as sequence of word indexes. Word is assigned an integer index based on its frequency. Higher the frequency lower the index. Index 0 is special index not assigned to particular word by to all unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words\n",
    "NUM_FEATURES = 25000\n",
    "\n",
    "#indices returned by keras have <START> and <UNKNOWN> as indexes 1 and 2. (And it assumes you will use 0 for <PADDING>).\n",
    "INDEX_FROM = 3 # This is anyway default parameter value. Explicitly mentioned here so that correcting word to index below is meaningful\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words = NUM_FEATURES, index_from=INDEX_FROM)\n",
    "\n",
    "# build index to word  from word to index.\n",
    "word_to_index = imdb.get_word_index()\n",
    "# Adjust word_to_index considering actual word starts from INDEX_FROM\n",
    "word_to_index = {k:(v+INDEX_FROM) for k,v in word_to_index.items()}\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<START>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "index_to_word = dict(map(reversed,word_to_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train  (25000,)\n",
      "Shape of y_train  (25000,)\n",
      "Shape of x_test  (25000,)\n",
      "Shape of y_test  (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x_train \", x_train.shape)\n",
    "print(\"Shape of y_train \", y_train.shape)\n",
    "print(\"Shape of x_test \", x_test.shape)\n",
    "print(\"Shape of y_test \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3:** Examine a move review by decoding from word indexes to english sentence. Also examine the sentiment label assigned to it - in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example of review of length  218  words, encoded with sequence of word indexes\n",
      " [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(\"An example of review of length \", len(x_train[0]) ,\" words, encoded with sequence of word indexes\\n\", x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Above review decoded into english words:\n",
      " ['<START>', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', \"redford's\", 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', \"norman's\", 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', 'retail', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', 'congratulations', 'to', 'the', 'two', 'little', \"boy's\", 'that', 'played', 'the', '<UNK>', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', 'praising', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(\"Above review decoded into english words:\\n\", list(map(lambda x: index_to_word[x], x_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of above example review is  Positive\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentiment of above example review is \", [\"Negative\", \"Positive\"][y_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4:** Truncate all reviews to be of same length\n",
    "\n",
    "Since keras or any neural nets does not take variable length inputs, truncate all reviews to be of same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 80 # 80 words in each review\n",
    "\n",
    "# since ending part of review has mostly the concluding remarks of review, \n",
    "# we are truncating the beginning part of movie review and keeping last 80 words\n",
    "x_train = sequence.pad_sequences(x_train, maxlen = MAX_LEN, padding='pre')\n",
    "x_test = sequence.pad_sequences(x_test, maxlen = MAX_LEN, padding='pre')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example truncated review decoded into english words from training data:\n",
      " ['that', 'played', 'the', '<UNK>', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', 'praising', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n"
     ]
    }
   ],
   "source": [
    "print(\"An example truncated review decoded into english words from training data:\\n\", list(map(lambda x: index_to_word[x], x_train[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example truncated review decoded into english words from test data:\n",
      " ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<START>', 'please', 'give', 'this', 'one', 'a', 'miss', 'br', 'br', 'kristy', 'swanson', 'and', 'the', 'rest', 'of', 'the', 'cast', 'rendered', 'terrible', 'performances', 'the', 'show', 'is', 'flat', 'flat', 'flat', 'br', 'br', 'i', \"don't\", 'know', 'how', 'michael', 'madison', 'could', 'have', 'allowed', 'this', 'one', 'on', 'his', 'plate', 'he', 'almost', 'seemed', 'to', 'know', 'this', \"wasn't\", 'going', 'to', 'work', 'out', 'and', 'his', 'performance', 'was', 'quite', 'lacklustre', 'so', 'all', 'you', 'madison', 'fans', 'give', 'this', 'a', 'miss']\n"
     ]
    }
   ],
   "source": [
    "print(\"An example truncated review decoded into english words from test data:\\n\", list(map(lambda x: index_to_word[x], x_test[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5:** Building a 3-layer Keras Model for sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5a** First layer - Embedding layer - to learn a good representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding of 128 dimensions for the NUM_FEATURES words\n",
    "DIMENSIONS = 128\n",
    "model.add(Embedding(input_dim=NUM_FEATURES, output_dim = DIMENSIONS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5b** Second layer - LSTM - to learn key idea within the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(DIMENSIONS, dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5c** Third layer - Dense - transforms the idea to sentiment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5d** Compile training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6** Train the network\n",
    "\n",
    "#### This step can be run multiple times - possibly it will increase accuracy if not flattened out already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previously saved model: ./imdb_lstm.h5\n",
      "Done loading model\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 230s 9ms/step - loss: 0.0761 - acc: 0.9744 - val_loss: 0.5737 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.57370, saving model to ./imdb_lstm.h5\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 246s 10ms/step - loss: 0.0660 - acc: 0.9773 - val_loss: 0.6742 - val_acc: 0.8137\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/10\n",
      "18240/25000 [====================>.........] - ETA: 53s - loss: 0.0436 - acc: 0.9873"
     ]
    }
   ],
   "source": [
    "model_filename = \"./imdb_lstm.h5\"\n",
    "\n",
    "if os.path.isfile(model_filename):\n",
    "    print(\"Loading previously saved model:\", model_filename)\n",
    "    model = load_model(model_filename)\n",
    "    print(\"Done loading model\")\n",
    "\n",
    "# Save model after each epoch.\n",
    "checkpoint_callback = callbacks.ModelCheckpoint(filepath=model_filename, verbose = 1, save_best_only= True)\n",
    "\n",
    "# Stop training when monitored quantity has stopped improving.\n",
    "# stop training if the val_loss has reduced (i.e delta is < 0) for 2 (patience) consecutive times\n",
    "earlystopping_callback = callbacks.EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 2, verbose = 1, mode='auto')\n",
    "\n",
    "model.fit(x=x_train, y=y_train, epochs=10, batch_size=64, \n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks = [checkpoint_callback, earlystopping_callback])\n",
    "\n",
    "loss,accuracy = model.evaluate(x=x_test, y=y_test,batch_size=64)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7:** Train using TF-IDF and LogRegression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics                 import accuracy_score,log_loss\n",
    "from sklearn.pipeline                import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.38315324286831653\n",
      "Accuracy: 0.84228\n"
     ]
    }
   ],
   "source": [
    "# convert integers in X_train as strings\n",
    "x_train_s = [' '.join(map(str, row)) for row in x_train]\n",
    "x_test_s  = [' '.join(map(str, row)) for row in x_test]\n",
    "\n",
    "pipeline = Pipeline([['counter', TfidfVectorizer()],\n",
    "                     ['classifier', LogisticRegression()]])\n",
    "\n",
    "pipeline.fit(x_train_s, y_train)\n",
    "\n",
    "y_predicted = pipeline.predict(x_test_s)\n",
    "\n",
    "# predicted probablity estimates.\n",
    "y_predicted_proba = pipeline.predict_proba(x_test_s)\n",
    "\n",
    "print(\"Loss:\", log_loss(y_test, y_predicted_proba))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Method       | Loss_Function      | Loss value|Accuracy|\n",
    "|--------------|:-------------------| --------: |-------:|\n",
    "| LSTM         | binary_crossentropy|   0.379   |  83.6  |\n",
    "| TF-IDF/LogReg| LogLoss            |   0.383   |  84.2  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Learning Resources\n",
    "1. State-of-art Sentiment classifier - Unsupervised Sentiment Neuron - https://blog.openai.com/unsupervised-sentiment-neuron/\n",
    "2. Understanding LSTM Networks https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
